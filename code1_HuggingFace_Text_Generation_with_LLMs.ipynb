{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Demystify_Generative_AI/blob/main/code1_HuggingFace_Text_Generation_with_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace: Using Pre-trained LLMs"
      ],
      "metadata": {
        "id": "IKB3HJRqq_M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai --quiet"
      ],
      "metadata": {
        "id": "lY71560SeO3H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"\""
      ],
      "metadata": {
        "id": "l5uAMFfSeQ9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "zky74m3zeUul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.responses.create(\n",
        "  model=\"gpt-4o\",\n",
        "  input=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},]\n",
        "  temperature=0.7\n",
        ")\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "id": "KRiYriq1eXX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using pretrained Models via APIs (OpenAI)"
      ],
      "metadata": {
        "id": "jh1vCXLFKz5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install accelerate torch --quiet\n",
        "!pip install sentencepiece --quiet"
      ],
      "metadata": {
        "id": "sbG1OGy0X8vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM: GPT-2\n",
        "\n",
        "Context: https://huggingface.co/openai-community/gpt2"
      ],
      "metadata": {
        "id": "hQUotuMgYaMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='openai-community/gpt2')\n",
        "set_seed(42)\n",
        "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
      ],
      "metadata": {
        "id": "0ggocMJ4_Ypx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Microsoft Phi2\n",
        "\n",
        "\n",
        "Phi-2 is a Transformer with 2.7 billion parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value). When assessed against benchmarks testing common sense, language understanding, and logical reasoning, Phi-2 showcased a nearly state-of-the-art performance among models with less than 13 billion parameters.\n",
        "\n",
        "\n",
        "HF: https://huggingface.co/microsoft/phi-2"
      ],
      "metadata": {
        "id": "u1QRBBpwYfdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "puu3YYoQYXKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load model to GPU (cuda) if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load tokenizer and model from Hugging Face\n",
        "model_id = \"microsoft/phi-2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if device == \"cuda\" else torch.float32)\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "j8RTPPb9fGen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your prompt\n",
        "prompt = \"Explain quantum computing in simple terms.\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "# Decode and print\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "PPWmMPh3gEz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Gemma 3 1B\n",
        "\n",
        "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\n",
        "\n",
        "HF: https://huggingface.co/google/gemma-3-1b-it"
      ],
      "metadata": {
        "id": "FfnYFpBik5t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model from Hugging Face\n",
        "model_id = \"google/gemma-3-1b-it\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if device == \"cuda\" else torch.float32)\n",
        "model.to(device)\n",
        "\n",
        "# Your prompt\n",
        "prompt = \"Explain quantum computing in simple terms.\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "# Decode and print\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "CXSq-LJIgfVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DlK9_csyK3N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtw9yI_Z-MQ7"
      },
      "source": [
        "## Thank You"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aB_4eDm3K5dP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}